from langchain.callbacks import get_openai_callback
from langchain.schema import HumanMessage, SystemMessage

import IRB_Assistant.prompts as irb_assistant_prompts
import IRB_Assistant_config.config as irb_assistant_config


def get_irb_assistant_response(
    hypothesis: str,
    inclusion: str,
    time_window: str,
    exclusion: str,
    design: str,
    details: str,
    chat=irb_assistant_config.CHAT,
    chat_prompt=irb_assistant_prompts.irb_chat_prompt,
):
    """
    The function `get_irb_assistant_response` takes in various parameters related to an IRB
    (Institutional Review Board) application and uses a chatbot to generate a response.

    Args:
      hypothesis (str): The hypothesis is a statement or prediction that you want to test or investigate
    in your research study. It is usually formulated as an if-then statement, where the "if" part
    represents the independent variable or condition, and the "then" part represents the expected
    outcome or dependent variable.
      inclusion (str): The "inclusion" parameter refers to the criteria that participants must meet in
    order to be included in the study. It specifies the characteristics or conditions that individuals
    must have in order to be eligible to participate.
      time_window (str): The `time_window` parameter is used to specify the time period or duration for
    which the research study will be conducted. It can be a specific time frame (e.g., "6 months", "1
    year") or a more general description (e.g., "short-term", "long-term").
      exclusion (str): The "exclusion" parameter refers to any criteria or factors that would exclude
    certain participants or data from your study. This could include things like age restrictions,
    medical conditions, or other specific characteristics that would make someone ineligible to
    participate or have their data included in the study.
      design (str): The "design" parameter refers to the design of a research study. It typically
    includes information about the methodology, data collection procedures, and analysis techniques that
    will be used in the study.
      details (str): The "details" parameter is a string that contains additional information or
    specifications related to the research study. It can include things like the specific variables
    being measured, the sample size, the research methodology, or any other relevant details that help
    provide context for the study.
      chat: The `chat` parameter is a function that takes a chat prompt as input and returns a response
    from the IRB assistant. It is used to communicate with the assistant during the conversation.
      chat_prompt: The `chat_prompt` parameter is a string that contains a placeholder for the question,
    inclusion criteria, time window, exclusion criteria, design, and details. These placeholders are
    represented by the variables `{question}`, `{inclusion}`, `{time_window}`, `{exclusion}`,
    `{design}`, and `{details}`

    Returns:
      the response from the chatbot assistant.
    """

    response = chat(
        chat_prompt.format_prompt(
            question=hypothesis,
            inclusion=inclusion,
            time_window=time_window,
            exclusion=exclusion,
            design=design,
            details=details,
        ).to_messages()
    )
    return response


def get_variable_assistant_response(
    generated_protocol: str,
    chat=irb_assistant_config.CHAT,
    chat_prompt=irb_assistant_prompts.variable_chat_prompt,
):
    """
    The function `get_variable_assistant_response` takes a generated protocol as input and uses a chat
    assistant to generate a response.

    Args:
      generated_protocol (str): A string representing the generated protocol that you want to pass to
    the chat assistant.
      chat: The `chat` parameter is a function that takes a list of messages as input and returns a list
    of responses. It represents the chatbot or assistant that you are using to generate responses.
      chat_prompt: The `chat_prompt` parameter is a string that contains a placeholder `{protocol}`.
    This placeholder is used to insert the generated protocol into the prompt. The `format_prompt`
    method is called on the `chat_prompt` to replace the placeholder with the generated protocol.

    Returns:
      The function `get_variable_assistant_response` returns the response generated by the chat
    assistant when given a generated protocol as input.
    """

    response = chat(chat_prompt.format_prompt(protocol=generated_protocol).to_messages())
    return response


def generate_search_string(input_research_q, loop_n=0, last_query=""):
    """
    The `generate_search_string` function takes an input research question, an optional loop number, and
    an optional last query. It generates a search string using OpenAI's ChatGPT model and returns the
    generated search string.

    Args:
      input_research_q: The input research question that you want to generate a search string for.
      loop_n: The `loop_n` parameter is used to keep track of the number of times the search query has
    been looped. It is an optional parameter with a default value of 0. Defaults to 0
      last_query: The `last_query` parameter is a string that represents the previous search query that
    was executed. It is used in the `generate_search_string` function to provide context to the AI model
    about the previous search results. This can be helpful when the previous search did not yield many
    results and the user wants

    Returns:
      The function `generate_search_string` returns a string that is generated by the OpenAI ChatGPT
    model.
    """
    prompt = irb_assistant_prompts.PUBMED_PROMPT.format(input_research_q)
    chat = irb_assistant_config.PUBMED_CHAT
    if loop_n > 0:
        prompt = prompt + irb_assistant_prompts.FEW_RESULTS_PROMPT + last_query
    with get_openai_callback() as response_meta:
        messages = [
            SystemMessage(
                content="You are an expert at conducting medical literature searches. You help beginning researchers."
            ),
            HumanMessage(content=prompt),
        ]
        response = chat(messages)

    return response.content, response_meta


def generate_overall_introduction(question, abstracts, help_type):
    """
    The function `generate_overall_introduction` takes a question, a list of abstracts, and a help type
    as input, and generates an overall introduction by summarizing the literature related to the
    question.

    Args:
      question: The question you want to generate an overall introduction for.
      abstracts: The `abstracts` parameter is a list of abstracts from relevant literature sources.
    These abstracts provide a summary of the research papers or articles related to the given question.
      help_type: The `help_type` parameter is used to specify the type of help you are seeking. It can
    be a string that describes the specific type of assistance you need, such as "clarification",
    "summary", "analysis", or any other relevant description. This helps the AI understand the context
    and provide

    Returns:
      The function `generate_overall_introduction` returns the overall introduction generated by the AI
    model.
    """
    prompt = irb_assistant_prompts.SUMMARIZE_LITERATURE_PROMPT.format(
        question, help_type, abstracts
    )
    chat = irb_assistant_config.CHAT
    with get_openai_callback() as response_meta:
        messages = [
            SystemMessage(
                content="You are an expert at conducting medical literature searches. You help beginning researchers."
            ),
            HumanMessage(content=prompt),
        ]
        response = chat(messages)

    return response.content, response_meta


def generate_simplified_text(
    complex_text: str,
    chat=irb_assistant_config.CHAT,
    chat_prompt=irb_assistant_prompts.simplify_chat_prompt,
):
    """
    The function `generate_simplified_text` takes a complex text as input and uses a chat model to
    generate a simplified version of the text.

    Args:
      complex_text (str): The `complex_text` parameter is a string that represents the text that you
    want to simplify. It is the input text that you want to pass to the `generate_simplified_text`
    function.
      chat: The `chat` parameter is a function that takes a prompt as input and returns a response. It
    is used to interact with the chat model.
      chat_prompt: The `chat_prompt` parameter is a string that represents the prompt to be used when
    generating a response from the chat model. It typically includes placeholders for the input text,
    which will be replaced with the actual input text when generating the prompt.

    Returns:
      The function `generate_simplified_text` returns the response generated by the chat model when
    given the complex text as input.
    """
    with get_openai_callback() as response_meta:
        response = chat(chat_prompt.format_prompt(text=complex_text).to_messages())
    return response, response_meta
